{
  "app": {
    "title": "Meeting Audio Transcription",
    "subtitle": "Upload audio files and transcribe using Azure Speech or Whisper"
  },
  "upload": {
    "title": "Upload & Configure",
    "audioFile": "Audio File:",
    "fileSelected": "Selected: {{filename}}",
    "method": "Transcription Method:",
    "whisperModel": "Whisper Model:",
    "language": "Language (optional):",
    "languagePlaceholder": "e.g., en-US, es-ES (leave empty for auto-detect)",
    "languageCandidates": "Multi-language Support (optional):",
    "languageCandidatesPlaceholder": "e.g., en-US,nl-NL for Dutch with English",
    "languageCandidatesHelp": "Enter comma-separated language codes for mixed-language transcription (Azure only)",
    "customTerms": "Custom Terms (optional):",
    "customTermsPlaceholder": "Enter custom terms separated by commas or new lines\ne.g., Kubernetes, Azure DevOps, MLOps",
    "customTermsHelp": "Add technical terms, proper nouns, or domain-specific vocabulary to improve recognition accuracy",
    "termsFile": "Or Upload Terms File (optional):",
    "termsFileSelected": "Terms file: {{filename}}",
    "termsFileHelp": "Upload a text file with one term per line",
    "enableDiarization": "Enable Speaker Diarization",
    "azureOnly": "(Azure only)",
    "enableNlp": "Enable NLP Analysis (key phrases, sentiment, etc.)",
    "uploadButton": "Transcribe",
    "uploadingButton": "Uploading...",
    "formLabel": "Transcription configuration form",
    "azureSectionTitle": "üéô Azure AI Speech Settings",
    "whisperSectionTitle": "ü§ñ Azure Whisper Settings",
    "nlpSectionTitle": "üìä NLP Analysis Settings",
    "profanityFilter": "Profanity Filter",
    "profanityMasked": "Masked (****)",
    "profanityRemoved": "Removed",
    "profanityRaw": "Raw (no filter)",
    "maxSpeakers": "Max Speakers",
    "maxSpeakersHelp": "Maximum number of speakers to detect (1-36)",
    "wordLevelTimestamps": "Enable word-level timestamps",
    "temperature": "Temperature",
    "temperatureHelp": "0 = deterministic, higher = more creative. Lower values recommended for transcription.",
    "initialPrompt": "Initial Prompt",
    "initialPromptPlaceholder": "Optional text to guide the model's style or continue a previous segment...",
    "initialPromptHelp": "Guide the transcription style. Include terms, acronyms, or context. Max ~224 tokens ({{count}}/800 characters).",
    "summaryLength": "Summary Length",
    "summaryLengthValue": "{{count}} sentences",
    "nlpSegmentSentiment": "Per-Segment Sentiment",
    "sentimentThreshold": "Sentiment Confidence Threshold",
    "sentimentThresholdHelp": "Default: 60%. Higher = only strong emotions shown, lower = more sensitive.",
    "nlpKeyPhrases": "Key Phrases",
    "nlpEntities": "Entity Recognition",
    "nlpActionItems": "Action Items",
    "nlpSummary": "Summary",
    "preprocessingSectionTitle": "üîä Audio Pre-processing",
    "audioChannels": "Channels",
    "channelsMono": "1 ‚Äì Mono (recommended)",
    "channelsStereo": "2 ‚Äì Stereo",
    "audioSampleRate": "Sample Rate",
    "audioBitRate": "Bit Rate",
    "hfSectionTitle": "ü§ó HuggingFace Wav2Vec 2.0 Settings",
    "hfModel": "Wav2Vec 2.0 Model",
    "hfModelBase": "Base English (960h)",
    "hfModelLarge": "Large English ‚Äì High Accuracy",
    "hfModelMultilingual": "Multilingual XLSR-53",
    "hfModelHelp": "Select a pre-trained model. Multilingual model supports 53 languages.",
    "hfUseApi": "Use Inference API / Foundry Endpoint",
    "hfEndpoint": "Custom Endpoint URL (optional)",
    "hfEndpointPlaceholder": "https://your-foundry-endpoint/score",
    "hfEndpointHelp": "Leave blank to use the HuggingFace Inference API. Set HUGGINGFACE_API_TOKEN on the server for authentication."
  },
  "tooltips": {
    "audioFile": "Choose an audio file from your computer (MP3, WAV, M4A, etc.). This is the recording you want to convert to text.",
    "method": "Pick how your audio gets converted to text. Azure AI Speech is best for multi-speaker meetings. Azure Whisper is great for general-purpose transcription.",
    "languageCandidates": "If your meeting has people speaking multiple languages, list the possible language codes here (e.g. en-US,nl-NL). The system will detect which language is being spoken in each part.",
    "profanityFilter": "Choose how swear words are handled: Masked replaces them with ****, Removed deletes them entirely, Raw keeps them as-is.",
    "maxSpeakers": "Set the maximum number of distinct speakers you expect in the recording. A higher number won't hurt accuracy but helps the system prepare for larger meetings.",
    "wordLevelTimestamps": "When enabled, each individual word gets its own timestamp ‚Äî useful if you need precise timing for captions or subtitles.",
    "temperature": "Controls randomness in the transcription. Keep it at 0 for the most accurate results. Higher values may produce varied but less reliable output.",
    "initialPrompt": "Give the AI context about your audio ‚Äî names of people, technical terms, or the topic being discussed. This helps it transcribe unfamiliar words correctly.",
    "language": "Specify the language spoken in the recording (e.g. en-US for American English). Leave empty to let the system detect it automatically.",
    "customTerms": "List specialised words, acronyms, or names that the AI might not recognise ‚Äî like product names, project codes, or technical jargon. One per line or comma-separated.",
    "termsFile": "Instead of typing custom terms one by one, upload a text file (.txt or .csv) containing your vocabulary list ‚Äî one term per line.",
    "enableDiarization": "Speaker diarization figures out \"who said what\" in your recording. It labels each part of the conversation with a speaker name. Essential for meetings with multiple participants.",
    "enableNlp": "Turns on smart text analysis after transcription. This extracts a summary, key phrases, sentiment (positive/negative tone), named entities, and action items from your meeting.",
    "summaryLength": "Controls how long the auto-generated summary will be. A higher number of sentences means a more detailed summary; lower means a brief overview.",
    "nlpSegmentSentiment": "Analyses the emotional tone of each individual speech segment, so you can see how sentiment changed throughout the meeting.",
    "sentimentThreshold": "Minimum confidence score required to label a segment as positive or negative. Segments below this threshold are marked as neutral. Higher values mean only strong sentiments are shown; lower values make detection more sensitive.",
    "nlpKeyPhrases": "Automatically extracts the most important words and phrases from the transcription ‚Äî great for a quick overview of what was discussed.",
    "nlpEntities": "Finds and labels people, organisations, locations, dates, and other named things mentioned in the meeting.",
    "nlpActionItems": "Detects commitments and to-dos mentioned during the meeting, like \"John will send the report by Friday\".",
    "nlpSummary": "Generates a concise summary of the entire meeting, capturing the main points discussed.",
    "audioChannels": "Number of audio channels. Use 1 (mono) for best speech recognition results. Use 2 (stereo) only if your recording has separate speaker channels.",
    "audioSampleRate": "Audio sample rate in Hz. 16 kHz is optimal for speech recognition. Higher rates preserve more audio detail but increase processing time and file size.",
    "audioBitRate": "Audio encoding bit rate. Lower values reduce file size, higher values preserve quality. 16k is sufficient for speech; use higher for music or high-fidelity recordings.",
    "hfModel": "HuggingFace model identifier. Use a pre-trained or fine-tuned Wav2Vec 2.0 model.",
    "hfUseApi": "Send audio to a remote HuggingFace Inference API or a Foundry-deployed endpoint instead of running locally.",
    "hfEndpoint": "Override the default HuggingFace Inference API URL with a custom Foundry-deployed endpoint."
  },
  "methods": {
    "azure": "Azure AI Speech",
    "whisper_api": "Azure OpenAI Whisper",
    "huggingface": "HuggingFace Wav2Vec 2.0"
  },
  "whisperModels": {
    "tiny": "Tiny (fastest, least accurate)",
    "base": "Base",
    "small": "Small",
    "medium": "Medium",
    "large": "Large (slowest, most accurate)"
  },
  "jobs": {
    "title": "Transcription Jobs",
    "noJobs": "No transcription jobs yet",
    "method": "Method:",
    "id": "ID:",
    "progress": "Progress:",
    "error": "Error:",
    "deleteButton": "Delete"
  },
  "status": {
    "pending": "pending",
    "processing": "processing",
    "completed": "completed",
    "failed": "failed"
  },
  "results": {
    "title": "Transcription Result:",
    "metadata": {
      "duration": "Duration:",
      "language": "Language:",
      "speakers": "Speakers:",
      "customTerms": "Custom Terms Applied:",
      "multiLanguage": "Multi-language:"
    },
    "segments": {
      "title": "Segments ({{count}}):",
      "more": "... and {{count}} more segments"
    },
    "nlp": {
      "title": "NLP Analysis",
      "summary": "Summary",
      "actionItems": "Action Items",
      "keyPhrases": "Key Phrases",
      "topics": "Topics",
      "sentiment": "Sentiment",
      "entities": "Entities",
      "noActionItems": "No action items detected",
      "noKeyPhrases": "No key phrases detected"
    }
  },
  "errors": {
    "noFile": "Please select an audio file",
    "uploadFailed": "Failed to submit transcription job"
  },
  "notifications": {
    "jobStarted": "Transcription started",
    "jobCompleted": "Transcription completed: {{filename}}",
    "jobFailed": "Transcription failed: {{filename}}",
    "jobDeleted": "Job deleted successfully",
    "jobDeleteFailed": "Failed to delete job"
  },
  "export": {
    "button": "Export",
    "exporting": "Exporting...",
    "error": "Export failed. Please try again.",
    "formats": {
      "txt": "Plain Text (.txt)",
      "docx": "Word Document (.docx)",
      "pdf": "PDF Document (.pdf)"
    }
  },
  "audio": {
    "play": "Play",
    "pause": "Pause",
    "seek": "Seek",
    "volume": "Volume"
  },
  "accessibility": {
    "skipToContent": "Skip to main content",
    "toggleControls": "Toggle accessibility controls",
    "settings": "Accessibility Settings",
    "title": "Accessibility",
    "fontSize": "Font Size",
    "fontNormal": "Normal",
    "fontLarge": "Large",
    "fontXLarge": "Extra Large",
    "highContrast": "High Contrast Mode",
    "keyboardInfo": "Use Tab to navigate, Enter to activate buttons"
  },
  "languageSelector": {
    "label": "Language:",
    "tooltip": "Select your preferred language"
  },
  "themes": {
    "light": "Light",
    "dark": "Dark",
    "ocean": "Ocean",
    "forest": "Forest",
    "sunset": "Sunset",
    "selectTheme": "Select theme",
    "chooseTheme": "Choose a Theme",
    "toggle": "Switch to {{mode}} mode"
  },
  "didYouKnow": {
    "title": "Did You Know?",
    "close": "Close",
    "fact1": "üåç There are over 7,000 languages spoken in the world today!",
    "fact2": "üé§ The first voice recording ever made was \"Mary had a little lamb\" in 1877!",
    "fact3": "ü§ñ AI transcription can now recognize speech in over 100 languages!",
    "fact4": "üó£Ô∏è Speaker diarization helps identify who said what in a conversation!",
    "fact5": "üéµ The human voice range is typically 85-255 Hz for males and 165-255 Hz for females!",
    "fact6": "üìù Transcription accuracy has improved by over 30% in the last 5 years!",
    "fact7": "üåè Mandarin Chinese is the most spoken native language with over 900 million speakers!",
    "fact8": "üîä Audio processing can remove background noise while preserving speech quality!",
    "fact9": "üí¨ The average person speaks at 110-150 words per minute!",
    "fact10": "üéôÔ∏è Professional transcriptionists can type 75-100 words per minute!",
    "fact11": "üß† NLP can extract sentiment and key phrases from transcribed text!",
    "fact12": "üåü Whisper AI can transcribe and translate simultaneously!",
    "fact13": "üìä Voice recognition technology dates back to the 1950s!",
    "fact14": "üéØ Custom vocabulary improves transcription accuracy by up to 15%!",
    "fact15": "üåà Some languages have sounds that don't exist in others!"
  },
  "badges": {
    "title": "Achievements",
    "viewBadges": "View badges",
    "earned": "Badge Earned!",
    "first_transcription": {
      "name": "First Steps",
      "description": "Complete your first transcription"
    },
    "five_transcriptions": {
      "name": "Getting Started",
      "description": "Complete 5 transcriptions"
    },
    "ten_transcriptions": {
      "name": "Professional",
      "description": "Complete 10 transcriptions"
    },
    "one_hour": {
      "name": "Hour Master",
      "description": "Transcribe 1 hour of audio"
    },
    "five_hours": {
      "name": "Time Keeper",
      "description": "Transcribe 5 hours of audio"
    },
    "multilingual": {
      "name": "Polyglot",
      "description": "Transcribe in 3 different languages"
    },
    "nlp_user": {
      "name": "NLP Explorer",
      "description": "Use NLP analysis 5 times"
    },
    "diarization_expert": {
      "name": "Speaker Pro",
      "description": "Use speaker diarization 10 times"
    }
  }
}
