{
  "app": {
    "title": "Trascrizione Audio di Riunioni",
    "subtitle": "Carica file audio e trascrivi usando Azure Speech o Whisper"
  },
  "upload": {
    "title": "Carica e Configura",
    "audioFile": "File Audio:",
    "fileSelected": "Selezionato: {{filename}}",
    "method": "Metodo di Trascrizione:",
    "whisperModel": "Modello Whisper:",
    "language": "Lingua (opzionale):",
    "languagePlaceholder": "es., en-US, es-ES (lasciare vuoto per rilevamento automatico)",
    "languageCandidates": "Supporto Multilingua (opzionale):",
    "languageCandidatesPlaceholder": "es., en-US,nl-NL per olandese con inglese",
    "languageCandidatesHelp": "Inserisci codici lingua separati da virgole per trascrizione multilingua (solo Azure)",
    "customTerms": "Termini Personalizzati (opzionale):",
    "customTermsPlaceholder": "Inserisci termini personalizzati separati da virgole o nuove righe\nes., Kubernetes, Azure DevOps, MLOps",
    "customTermsHelp": "Aggiungi termini tecnici, nomi propri o vocabolario specifico del dominio per migliorare la precisione del riconoscimento",
    "termsFile": "O Carica File di Termini (opzionale):",
    "termsFileSelected": "File di termini: {{filename}}",
    "termsFileHelp": "Carica un file di testo con un termine per riga",
    "enableDiarization": "Abilita Diarizzazione del Parlante",
    "azureOnly": "(solo Azure)",
    "enableNlp": "Abilita Analisi NLP (frasi chiave, sentiment, ecc.)",
    "uploadButton": "Trascrivi",
    "uploadingButton": "Caricamento...",
    "formLabel": "Transcription configuration form",
    "azureSectionTitle": "üéô Azure AI Speech Settings",
    "whisperSectionTitle": "ü§ñ Azure Whisper Settings",
    "nlpSectionTitle": "üìä NLP Analysis Settings",
    "profanityFilter": "Profanity Filter",
    "profanityMasked": "Masked (****)",
    "profanityRemoved": "Removed",
    "profanityRaw": "Raw (no filter)",
    "maxSpeakers": "Max Speakers",
    "maxSpeakersHelp": "Maximum number of speakers to detect (1-36)",
    "wordLevelTimestamps": "Enable word-level timestamps",
    "temperature": "Temperature",
    "temperatureHelp": "0 = deterministic, higher = more creative. Lower values recommended for transcription.",
    "initialPrompt": "Initial Prompt",
    "initialPromptPlaceholder": "Optional text to guide the model's style or continue a previous segment...",
    "initialPromptHelp": "Guide the transcription style. Include terms, acronyms, or context. Max ~224 tokens ({{count}}/800 characters).",
    "summaryLength": "Summary Length",
    "summaryLengthValue": "{{count}} sentences",
    "nlpSegmentSentiment": "Per-Segment Sentiment",
    "sentimentThreshold": "Sentiment Confidence Threshold",
    "sentimentThresholdHelp": "Default: 60%. Higher = only strong emotions shown, lower = more sensitive.",
    "nlpKeyPhrases": "Key Phrases",
    "nlpEntities": "Entity Recognition",
    "nlpActionItems": "Action Items",
    "nlpSummary": "Summary",
    "preprocessingSectionTitle": "üîä Audio Pre-processing",
    "audioChannels": "Channels",
    "channelsMono": "1 ‚Äì Mono (recommended)",
    "channelsStereo": "2 ‚Äì Stereo",
    "audioSampleRate": "Sample Rate",
    "audioBitRate": "Bit Rate",
    "hfSectionTitle": "ü§ó HuggingFace Wav2Vec 2.0 Settings",
    "hfModel": "Wav2Vec 2.0 Model",
    "hfModelBase": "Base English (960h)",
    "hfModelLarge": "Large English ‚Äì High Accuracy",
    "hfModelMultilingual": "Multilingual XLSR-53",
    "hfModelHelp": "Select a pre-trained model. Multilingual model supports 53 languages.",
    "hfUseApi": "Use Inference API / Foundry Endpoint",
    "hfEndpoint": "Custom Endpoint URL (optional)",
    "hfEndpointPlaceholder": "https://your-foundry-endpoint/score",
    "hfEndpointHelp": "Leave blank to use the HuggingFace Inference API. Set HUGGINGFACE_API_TOKEN on the server for authentication."
  },
  "methods": {
    "azure": "Azure Speech Services",
    "whisper_local": "Whisper (Locale)",
    "whisper_api": "Whisper (API OpenAI)",
    "huggingface": "HuggingFace Wav2Vec 2.0"
  },
  "whisperModels": {
    "tiny": "Tiny (pi√π veloce, meno preciso)",
    "base": "Base",
    "small": "Small",
    "medium": "Medium",
    "large": "Large (pi√π lento, pi√π preciso)"
  },
  "jobs": {
    "title": "Lavori di Trascrizione",
    "noJobs": "Nessun lavoro di trascrizione ancora",
    "method": "Metodo:",
    "id": "ID:",
    "progress": "Progresso:",
    "error": "Errore:",
    "deleteButton": "Elimina"
  },
  "status": {
    "pending": "in attesa",
    "processing": "in elaborazione",
    "completed": "completato",
    "failed": "fallito"
  },
  "results": {
    "title": "Risultato di Trascrizione:",
    "metadata": {
      "duration": "Durata:",
      "language": "Lingua:",
      "speakers": "Parlanti:",
      "customTerms": "Termini Personalizzati Applicati:",
      "multiLanguage": "Multilingua:"
    },
    "segments": {
      "title": "Segmenti ({{count}}):",
      "more": "... e altri {{count}} segmenti"
    },
    "nlp": {
      "title": "Analisi NLP:",
      "keyPhrases": "Frasi Chiave:",
      "sentiment": "Sentiment:",
      "summary": "Summary",
      "actionItems": "Action Items",
      "topics": "Topics",
      "entities": "Entities",
      "noActionItems": "No action items detected",
      "noKeyPhrases": "No key phrases detected"
    }
  },
  "errors": {
    "noFile": "Si prega di selezionare un file audio",
    "uploadFailed": "Impossibile inviare il lavoro di trascrizione"
  },
  "languageSelector": {
    "label": "Lingua:",
    "tooltip": "Seleziona la tua lingua preferita"
  },
  "tooltips": {
    "audioFile": "Choose an audio file from your computer (MP3, WAV, M4A, etc.). This is the recording you want to convert to text.",
    "method": "Pick how your audio gets converted to text. Azure AI Speech is best for multi-speaker meetings. Azure Whisper is great for general-purpose transcription.",
    "languageCandidates": "If your meeting has people speaking multiple languages, list the possible language codes here (e.g. en-US,nl-NL). The system will detect which language is being spoken in each part.",
    "profanityFilter": "Choose how swear words are handled: Masked replaces them with ****, Removed deletes them entirely, Raw keeps them as-is.",
    "maxSpeakers": "Set the maximum number of distinct speakers you expect in the recording. A higher number won't hurt accuracy but helps the system prepare for larger meetings.",
    "wordLevelTimestamps": "When enabled, each individual word gets its own timestamp ‚Äî useful if you need precise timing for captions or subtitles.",
    "temperature": "Controls randomness in the transcription. Keep it at 0 for the most accurate results. Higher values may produce varied but less reliable output.",
    "initialPrompt": "Give the AI context about your audio ‚Äî names of people, technical terms, or the topic being discussed. This helps it transcribe unfamiliar words correctly.",
    "language": "Specify the language spoken in the recording (e.g. en-US for American English). Leave empty to let the system detect it automatically.",
    "customTerms": "List specialised words, acronyms, or names that the AI might not recognise ‚Äî like product names, project codes, or technical jargon. One per line or comma-separated.",
    "termsFile": "Instead of typing custom terms one by one, upload a text file (.txt or .csv) containing your vocabulary list ‚Äî one term per line.",
    "enableDiarization": "Speaker diarization figures out \"who said what\" in your recording. It labels each part of the conversation with a speaker name. Essential for meetings with multiple participants.",
    "enableNlp": "Turns on smart text analysis after transcription. This extracts a summary, key phrases, sentiment (positive/negative tone), named entities, and action items from your meeting.",
    "summaryLength": "Controls how long the auto-generated summary will be. A higher number of sentences means a more detailed summary; lower means a brief overview.",
    "nlpSegmentSentiment": "Analyses the emotional tone of each individual speech segment, so you can see how sentiment changed throughout the meeting.",
    "sentimentThreshold": "Minimum confidence score required to label a segment as positive or negative. Segments below this threshold are marked as neutral. Higher values mean only strong sentiments are shown; lower values make detection more sensitive.",
    "nlpKeyPhrases": "Automatically extracts the most important words and phrases from the transcription ‚Äî great for a quick overview of what was discussed.",
    "nlpEntities": "Finds and labels people, organisations, locations, dates, and other named things mentioned in the meeting.",
    "nlpActionItems": "Detects commitments and to-dos mentioned during the meeting, like \"John will send the report by Friday\".",
    "nlpSummary": "Generates a concise summary of the entire meeting, capturing the main points discussed.",
    "audioChannels": "Number of audio channels. Use 1 (mono) for best speech recognition results. Use 2 (stereo) only if your recording has separate speaker channels.",
    "audioSampleRate": "Audio sample rate in Hz. 16 kHz is optimal for speech recognition. Higher rates preserve more audio detail but increase processing time and file size.",
    "audioBitRate": "Audio encoding bit rate. Lower values reduce file size, higher values preserve quality. 16k is sufficient for speech; use higher for music or high-fidelity recordings.",
    "hfModel": "HuggingFace model identifier. Use a pre-trained or fine-tuned Wav2Vec 2.0 model.",
    "hfUseApi": "Send audio to a remote HuggingFace Inference API or a Foundry-deployed endpoint instead of running locally.",
    "hfEndpoint": "Override the default HuggingFace Inference API URL with a custom Foundry-deployed endpoint."
  },
  "notifications": {
    "jobStarted": "Transcription started",
    "jobCompleted": "Transcription completed: {{filename}}",
    "jobFailed": "Transcription failed: {{filename}}",
    "jobDeleted": "Job deleted successfully",
    "jobDeleteFailed": "Failed to delete job"
  },
  "export": {
    "button": "Export",
    "exporting": "Exporting...",
    "error": "Export failed. Please try again.",
    "formats": {
      "txt": "Plain Text (.txt)",
      "docx": "Word Document (.docx)",
      "pdf": "PDF Document (.pdf)"
    }
  },
  "audio": {
    "play": "Play",
    "pause": "Pause",
    "seek": "Seek",
    "volume": "Volume"
  },
  "accessibility": {
    "skipToContent": "Skip to main content",
    "toggleControls": "Toggle accessibility controls",
    "settings": "Accessibility Settings",
    "title": "Accessibility",
    "fontSize": "Font Size",
    "fontNormal": "Normal",
    "fontLarge": "Large",
    "fontXLarge": "Extra Large",
    "highContrast": "High Contrast Mode",
    "keyboardInfo": "Use Tab to navigate, Enter to activate buttons"
  },
  "themes": {
    "light": "Light",
    "dark": "Dark",
    "ocean": "Ocean",
    "forest": "Forest",
    "sunset": "Sunset",
    "selectTheme": "Select theme",
    "chooseTheme": "Choose a Theme",
    "toggle": "Switch to {{mode}} mode"
  },
  "didYouKnow": {
    "title": "Did You Know?",
    "close": "Close",
    "fact1": "üåç There are over 7,000 languages spoken in the world today!",
    "fact2": "üé§ The first voice recording ever made was \"Mary had a little lamb\" in 1877!",
    "fact3": "ü§ñ AI transcription can now recognize speech in over 100 languages!",
    "fact4": "üó£Ô∏è Speaker diarization helps identify who said what in a conversation!",
    "fact5": "üéµ The human voice range is typically 85-255 Hz for males and 165-255 Hz for females!",
    "fact6": "üìù Transcription accuracy has improved by over 30% in the last 5 years!",
    "fact7": "üåè Mandarin Chinese is the most spoken native language with over 900 million speakers!",
    "fact8": "üîä Audio processing can remove background noise while preserving speech quality!",
    "fact9": "üí¨ The average person speaks at 110-150 words per minute!",
    "fact10": "üéôÔ∏è Professional transcriptionists can type 75-100 words per minute!",
    "fact11": "üß† NLP can extract sentiment and key phrases from transcribed text!",
    "fact12": "üåü Whisper AI can transcribe and translate simultaneously!",
    "fact13": "üìä Voice recognition technology dates back to the 1950s!",
    "fact14": "üéØ Custom vocabulary improves transcription accuracy by up to 15%!",
    "fact15": "üåà Some languages have sounds that don't exist in others!"
  },
  "badges": {
    "title": "Achievements",
    "viewBadges": "View badges",
    "earned": "Badge Earned!",
    "first_transcription": {
      "name": "First Steps",
      "description": "Complete your first transcription"
    },
    "five_transcriptions": {
      "name": "Getting Started",
      "description": "Complete 5 transcriptions"
    },
    "ten_transcriptions": {
      "name": "Professional",
      "description": "Complete 10 transcriptions"
    },
    "one_hour": {
      "name": "Hour Master",
      "description": "Transcribe 1 hour of audio"
    },
    "five_hours": {
      "name": "Time Keeper",
      "description": "Transcribe 5 hours of audio"
    },
    "multilingual": {
      "name": "Polyglot",
      "description": "Transcribe in 3 different languages"
    },
    "nlp_user": {
      "name": "NLP Explorer",
      "description": "Use NLP analysis 5 times"
    },
    "diarization_expert": {
      "name": "Speaker Pro",
      "description": "Use speaker diarization 10 times"
    }
  }
}
